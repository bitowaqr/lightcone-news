import { defineEventHandler, createError, readBody } from 'h3';
import Article from '../models/Article.model';
// import { askPerplexity } from '../agents/askPerplexity';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { checkRateLimit } from '../utils/rateLimit';

export default defineEventHandler(async (event) => {
  // Define rate limits
  const chatLimits = [
    { name: 'Hourly', windowMs: 60 * 60 * 1000, maxRequests: 100 }, // 100 requests per hour
    { name: 'Daily', windowMs: 24 * 60 * 60 * 1000, maxRequests: 1000 } // 1000 requests per day
  ];

  // Apply rate limiting
  try {
    await checkRateLimit(event, {
      limits: chatLimits, // Pass the array of limits
      endpointName: 'chat' // Base name for storage keys
    });
  } catch (error) {
    // Handle potential errors (e.g., 429 Too Many Requests)
    if (error.statusCode === 429) {
      console.log('rate limit error', error);
      return error.message;
    } else {
      // Log unexpected errors
      console.error("Unexpected error during rate limit check:", error);
      // Return a generic server error
      return createError({ statusCode: 500, statusMessage: 'Internal Server Error' });
    }
  }

  // Set the Content-Type header for streaming text
  event.node.res.setHeader('Content-Type', 'text/plain');
  event.node.res.setHeader('Transfer-Encoding', 'chunked');

  const MODEL = 'gemini-2.0-flash';
  const model = new ChatGoogleGenerativeAI({
    model: MODEL ,
    apiKey: process.env.GEMINI_API_KEY,
    temperature: 0.3,
    maxRetries: 3,
  }).bindTools([{ googleSearch: {} }]);


  const body = await readBody(event);
  const { prompt, contextId, history } = body;

  console.log('history', history);

  if (!prompt || !contextId) {
    // We need to end the response here since we already set headers
    event.node.res.statusCode = 400;
    event.node.res.statusMessage = 'Bad Request: Missing prompt or contextId';
    return event.node.res.end(JSON.stringify({
      statusCode: 400,
      statusMessage: 'Bad Request: Missing prompt or contextId'
    }));
  }

  const article = await Article.findById(contextId);
  // Handle case where article is not found
  if (!article) {
      event.node.res.statusCode = 404;
      event.node.res.statusMessage = `Not Found: Article with contextId ${contextId} not found`;
      return event.node.res.end(JSON.stringify({
          statusCode: 404,
          statusMessage: `Not Found: Article with contextId ${contextId} not found`
      }));
  }

  const systemPrompt = `<context>
  Primary role: You are the AI assistant integrated into Lightcone.news article pages. Your primary function is to help users understand the specific article they are viewing by providing additional context, answering questions about its content, sources, timeline, and related future scenarios.

  Secondary Role: Users might also ask you questions about Lightcone.news itself. Use the following information to answer those queries accurately and helpfully.

  <communucation-directives>
  Your Identity (When Asked "Who are you?" or similar):
  - "I am an AI assistant integrated into Lightcone.news. My purpose is to help you explore the context, history, and potential future developments related to the news stories presented here. I can answer questions about the article you're currently reading."
  - "I'm the Lightcone.news AI chat feature. You can ask me questions to get more details or clarification on this article, its timeline, sources, or the future scenarios discussed."
  </communucation-directives>

  <lightcone-news-background>

  If user asks about Lightcone.news, you can use the following information to answer those queries accurately and helpfully:

  "Lightcone.news is an AI-augmented online news aggregation platform designed to provide a contextualised, future-oriented news feed."

  Answering "Who is behind Lightcone.news?" or "Who made this?":
  "Lightcone.news is currently being developed. You can reach out to Paul Schneider at paul@priorb.com"

  Answering Questions About Specific Features (e.g., Timelines, Scenarios):

  * Timelines: "The timeline feature provides historical context, showing the key events leading up to the current situation discussed in the article. It's generated by AI analysis of relevant source materials."
  * Scenarios/Forecasts: "The future scenarios section explores potential future developments related to the news story. These are often based on probabilistic forecasts from prediction platforms or generated by AI analysis, aiming to outline plausible outcomes and their likelihood."
  * Sources: "We list the primary sources used for the article's information and context. Transparency about our sources is important."
  
  *If users ask for a way to contact the Lightcone.news team, provide the contact information: "You can reach out to Paul Schneider at paul@priorb.com"

  </lightcone-news-background>

  <tone-and-style>
  * Be Helpful and Informative: Your main goal is to clarify and provide useful information.
  * Be Objective and Factual: Stick to the information provided here about the platform's purpose and features.
  * Be Clear and Concise: Avoid overly technical jargon unless necessary.
  * Be Engaging: Encourage users to explore the platform's features.
  * Use your search tool to verify any factual information you provide, but do not provide links in your response, unless specifically asked for.

  </tone-and-style>

  <article>
  The user is currently reading the following news article on Lightcone.news:
  Title: ${article.title}
  Precis: ${article.precis}
  Summary: ${article.summary}
  Timeline: ${JSON.stringify(article.timeline)}

  You can assume that the user has read the article, and is asking you follow-up questions about the article, or seeking additional context to better understand the article, or to seek additional information about related topics, historical context, etc.
  </article>

  <user-interaction>
  The user is engaging with you through a chat interface directly on the article page.
  </user-interaction>

  </context>
  
  <instructions>
  * Be extremely concise and to the point: 1-3 sentence responses are optimal, unless the user specifically asks for more extended information or the query clearly demands a more comprehensive response.
  * Make use of your search tool to verify any factual information you provide, but do not provide links in your response, unless specifically asked for.
  <limitations>
  * Do no engage with the user about anything else than the article, or topics related to the article, geopolitical news, lightcone.news, or anything related to the user's reading experience on lightcone.news.
  * If the user asks you to do anything else, you must politely decline to avoid violating the terms of service. 
  * Examples of unacceptable requests: "What is the weather in Tokyo?", "What is the capital of France?", "What is the meaning of life?", "write a poem about the user's pet turtle", etc.
  * Whenever you are unsure if something is or is not acceptable, err on the side of caution, decline to engage, and note that 'you are still learning' - apologise and promise to get better over time.
  </limitations>
  </instructions>`;

  const chatHistory = []
  history.forEach(message => {
    if(!message.text || message.text.trim() === '') return;
    chatHistory.push({
      role: message.sender === 'user' ? 'user' : 'assistant',
      content: message.text
    });
  });

  const messages = [
    { role: 'system', content: systemPrompt },
    ...chatHistory]
  
  try {
    const stream = await model.stream(messages);

    // Stream the response chunks
    for await (const chunk of stream) {
      if (chunk?.content) {
        event.node.res.write(chunk.content);
      }
    }
  } catch (error) {
      console.error('LLM stream error:', error);
      // Ensure the response is ended even if there's an error during streaming
      if (!event.node.res.writableEnded) {
          event.node.res.statusCode = 500;
          event.node.res.statusMessage = 'Internal Server Error: Failed to get response from LLM';
          event.node.res.end(JSON.stringify({
              statusCode: 500,
              statusMessage: 'Internal Server Error: Failed to get response from LLM'
          }));
      }
      // We don't want to proceed further after handling the error
      return;
  }


  // End the response stream
  event.node.res.end();

  // IMPORTANT: Return nothing here, as the response is handled by the stream.
  // Returning anything would cause an error because headers are already sent.
}); 