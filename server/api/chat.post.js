import { defineEventHandler, createError, readBody } from 'h3';
import Article from '../models/Article.model';
// import { askPerplexity } from '../agents/askPerplexity';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { checkRateLimit } from '../utils/rateLimit';

export default defineEventHandler(async (event) => {
  // Define rate limits
  const chatLimits = [
    { name: 'Hourly', windowMs: 60 * 60 * 1000, maxRequests: 100 }, // 100 requests per hour
    { name: 'Daily', windowMs: 24 * 60 * 60 * 1000, maxRequests: 1000 } // 1000 requests per day
  ];

  // Apply rate limiting
  try {
    await checkRateLimit(event, {
      limits: chatLimits, // Pass the array of limits
      endpointName: 'chat' // Base name for storage keys
    });
  } catch (error) {
    // Handle potential errors (e.g., 429 Too Many Requests)
    if (error.statusCode === 429) {
      console.log('rate limit error', error);
      return error.message;
    } else {
      // Log unexpected errors
      console.error("Unexpected error during rate limit check:", error);
      // Return a generic server error
      return createError({ statusCode: 500, statusMessage: 'Internal Server Error' });
    }
  }

  // Set the Content-Type header for ndjson streaming
  event.node.res.setHeader('Content-Type', 'application/x-ndjson');
  event.node.res.setHeader('Transfer-Encoding', 'chunked');

  const MODEL = 'gemini-2.0-flash';
  const model = new ChatGoogleGenerativeAI({
    model: MODEL ,
    apiKey: process.env.GEMINI_API_KEY,
    temperature: 0.3,
    maxRetries: 3,
  }).bindTools([{ googleSearch: {} }]);


  const body = await readBody(event);
  const { prompt, contextId, history } = body;

  if (!prompt || !contextId) {
    // We need to end the response here since we already set headers
    event.node.res.statusCode = 400;
    event.node.res.statusMessage = 'Bad Request: Missing prompt or contextId';
    return event.node.res.end(JSON.stringify({
      statusCode: 400,
      statusMessage: 'Bad Request: Missing prompt or contextId'
    }));
  }

  const article = await Article.findById(contextId);
  // Handle case where article is not found
  if (!article) {
      event.node.res.statusCode = 404;
      event.node.res.statusMessage = `Not Found: Article with contextId ${contextId} not found`;
      return event.node.res.end(JSON.stringify({
          statusCode: 404,
          statusMessage: `Not Found: Article with contextId ${contextId} not found`
      }));
  }

  const systemPrompt = `<context>
  Primary role: You are the AI assistant integrated into Lightcone.news article pages. Your primary function is to help users understand the specific article they are viewing by providing additional context, answering questions about its content, sources, timeline, and related future scenarios.

  Secondary Role: Users might also ask you questions about Lightcone.news itself. Use the following information to answer those queries accurately and helpfully.

  <communucation-directives>
  Your Identity (When Asked "Who are you?" or similar):
  - "I am an AI assistant integrated into Lightcone.news. My purpose is to help you explore the context, history, and potential future developments related to the news stories presented here. I can answer questions about the article you're currently reading."
  - "I'm the Lightcone.news AI chat feature. You can ask me questions to get more details or clarification on this article, its timeline, sources, or the future scenarios discussed."
  </communucation-directives>

  <lightcone-news-background>

  If user asks about Lightcone.news, you can use the following information to answer those queries accurately and helpfully:

  "Lightcone.news is an AI-augmented online news aggregation platform designed to provide a contextualised, future-oriented news feed."

  Answering "Who is behind Lightcone.news?" or "Who made this?":
  "Lightcone.news is currently being developed. You can reach out to Paul Schneider at paul@priorb.com"

  Answering Questions About Specific Features (e.g., Timelines, Scenarios):

  * Timelines: "The timeline feature provides historical context, showing the key events leading up to the current situation discussed in the article. It's generated by AI analysis of relevant source materials."
  * Scenarios/Forecasts: "The future scenarios section explores potential future developments related to the news story. These are often based on probabilistic forecasts from prediction platforms or generated by AI analysis, aiming to outline plausible outcomes and their likelihood."
  * Sources: "We list the primary sources used for the article's information and context. Transparency about our sources is important."
  
  *If users ask for a way to contact the Lightcone.news team, provide the contact information: "You can reach out to Paul Schneider at paul@priorb.com"

  </lightcone-news-background>

  <tone-and-style>
  * Be Helpful and Informative: Your main goal is to clarify and provide useful information.
  * Be Objective and Factual: Stick to the information provided here about the platform's purpose and features.
  * Be Clear and Concise: Avoid overly technical jargon unless necessary.
  * Be Engaging: Encourage users to explore the platform's features.
  </tone-and-style>

  <article>
  The user is currently reading the following news article on Lightcone.news:
  Title: ${article.title}
  Precis: ${article.precis}
  Summary: ${article.summary}
  Timeline: ${JSON.stringify(article.timeline)}

  You can assume that the user has read the article, and is asking you follow-up questions about the article, or seeking additional context to better understand the article, or to seek additional information about related topics, historical context, etc.
  </article>

  <user-interaction>
  The user is engaging with you through a chat interface directly on the article page. Your response will be displayed in a 'Response' container on the page. Underneath the 'Response' container, there is a 'Sources' container that displays the sources used to generate the response. Users can ask follow-up questions.
  </user-interaction>

  </context>
  
  <instructions>
  * Be concise and to the point: 1-3 short paragraphs responses are optimal, unless the user specifically asks for more extended information or the query clearly demands a more comprehensive response.
  * Use your search tool to verify any factual information you provide, but NEVER cite the source in your response.
  * Only use extremely trustworthy, high quality, and reputable sources.
  <limitations>
  * Do no engage with the user about anything else than the article, or topics related to the article, geopolitical news, lightcone.news, or anything related to the user's reading experience on lightcone.news.
  * If the user asks you to do anything else, you must politely decline to avoid violating the terms of service. 
  * Examples of unacceptable requests: "What is the weather in Tokyo?", "What is the capital of France?", "What is the meaning of life?", "write a poem about the user's pet turtle", etc.
  * Whenever you are unsure if something is or is not acceptable, err on the side of caution, decline to engage, and note that 'you are still learning' - apologise and promise to get better over time.
  </limitations>
  </instructions>`;

  const chatHistory = []
  history?.forEach(message => {
    if(!message.text || message.text.trim() === '') return;
    chatHistory.push({
      role: message.sender === 'user' ? 'user' : 'assistant',
      content: message.text
    });
  });

  const messages = [
    { role: 'system', content: systemPrompt },
    ...chatHistory
  ];
  

  // Check if messages array is valid (at least system + user/assistant message)
  if (messages.length < 2) {
      console.error('Error: Not enough messages to send to LLM. History might be missing the latest user prompt.');
      event.node.res.statusCode = 400;
      event.node.res.statusMessage = 'Bad Request: Invalid chat history provided.';
      return event.node.res.end(JSON.stringify({
          statusCode: 400,
          statusMessage: 'Bad Request: Invalid chat history provided.'
      }));
  }

  try {
    const stream = await model.stream(messages);

    // Helper function to safely write JSON lines
    const writeJsonLine = (data) => {
        try {
            event.node.res.write(JSON.stringify(data) + '\n');
        } catch (e) {
            console.error('Error writing JSON line to stream:', e);
            // Handle potential write errors (e.g., client disconnected)
            // You might want to break the loop or handle cleanup here
        }
    };

    // Stream the response chunks
    for await (const chunk of stream) {
      // console.log('Raw Chunk:', JSON.stringify(chunk, null, 2)); // DEBUG: Log the raw chunk structure

      // Send content chunk
      // console.log('Chunk:', chunk); // Keep or remove based on preference
      if (chunk?.content && typeof chunk.content === 'string' && chunk.content.trim() !== '') {
        writeJsonLine({ type: 'content', data: chunk.content });
      }

      // --- Extract and send sources from Grounding Metadata --- 
      let sources = [];
      const groundingChunks = chunk.response_metadata?.groundingMetadata?.groundingChunks;

      if (groundingChunks && Array.isArray(groundingChunks)) {
          // console.log('Found Grounding Chunks:', JSON.stringify(groundingChunks, null, 2)); // DEBUG: Log structure if needed
          for (const groundingChunk of groundingChunks) {
              // *** Corrected Structure: { web: { uri, title } } ***
              const result = groundingChunk?.web; // Access the 'web' property
              if (result?.uri) { // Check for 'uri' instead of 'link'
                  sources.push({
                      url: result.uri, // Use uri
                      title: result.title || '' // Keep title
                      // snippet: result.snippet || '' // Snippet doesn't seem available here
                  });
              } else {
                  // Fallback or alternative structure check if needed
                  console.log('Could not find web.uri in:', groundingChunk); // Updated debug log
              }
          }
      }

      // Send sources if any were found in this chunk
      if (sources.length > 0) {
          // Deduplicate sources based on URL before sending
          const uniqueSources = Array.from(new Map(sources.map(s => [s.url, s])).values());
           // Only send if there are actually sources to send
           if (uniqueSources.length > 0) {
              // console.log('Sending sources chunk:', JSON.stringify({ type: 'sources', data: uniqueSources })); // DEBUG: Log sources being sent
              writeJsonLine({ type: 'sources', data: uniqueSources });
           }
      }
      // --- End source extraction --- 

      // --- REMOVE OLD Tool Call Logic --- 
      /* 
      let sources_old = [];
      // Check standard tool_calls format (might appear in final chunk)
      if (chunk?.additional_kwargs?.tool_calls) { ... }
      // Check tool_call_chunks format (might appear incrementally)
      if (chunk?.tool_call_chunks) { ... }
      // Send sources if any were found in this chunk
      if (sources_old.length > 0) { ... }
      */
      // --- End REMOVE OLD Tool Call Logic --- 

    }
  } catch (error) {
      console.error('LLM stream error:', error);
      // Ensure the response is ended even if there's an error during streaming
      if (!event.node.res.writableEnded) {
          event.node.res.statusCode = 500;
          event.node.res.statusMessage = 'Internal Server Error: Failed to get response from LLM';
          event.node.res.end(JSON.stringify({
              statusCode: 500,
              statusMessage: 'Internal Server Error: Failed to get response from LLM'
          }));
      }
      // We don't want to proceed further after handling the error
      return;
  }


  // End the response stream
  event.node.res.end();

  // IMPORTANT: Return nothing here, as the response is handled by the stream.
  // Returning anything would cause an error because headers are already sent.
}); 